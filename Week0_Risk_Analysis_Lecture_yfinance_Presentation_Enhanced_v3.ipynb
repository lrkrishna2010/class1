{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216444d0",
   "metadata": {},
   "source": [
    "\n",
    "# 0. Module 0: Prerequisites and Setup\n",
    "\n",
    "This module ensures students have the necessary technical and theoretical background.\n",
    "\n",
    "---\n",
    "\n",
    "## 0.1 Technical Prerequisite: Python Foundations\n",
    "\n",
    "A core component of quantitative research is the ability to handle and process large financial datasets efficiently. We rely heavily on the following libraries:\n",
    "\n",
    "1. **Pandas:** For managing time-series data using DataFrames.  \n",
    "2. **NumPy:** For high-performance numerical operations and vectorization.  \n",
    "3. **Matplotlib/Seaborn:** For data visualization.\n",
    "\n",
    "### Review Exercise: Basic Data Manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2807ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a basic DataFrame\n",
    "dates = pd.date_range('2024-01-01', periods=5)\n",
    "data = {'Price': [100, 102, 99, 105, 104], 'Volume': [1000, 1500, 800, 2000, 1200]}\n",
    "df_ex = pd.DataFrame(data, index=dates)\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "print(df_ex)\n",
    "\n",
    "# Quick review of calculating log returns and basic stats\n",
    "log_returns_ex = np.log(df_ex['Price'] / df_ex['Price'].shift(1)).dropna()\n",
    "\n",
    "print(\"\\nLog Returns (first 4):\")\n",
    "print(log_returns_ex)\n",
    "print(f\"\\nMean Return: {log_returns_ex.mean():.4f}\")\n",
    "print(f\"Standard Deviation (Risk): {log_returns_ex.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af6fa3",
   "metadata": {},
   "source": [
    "\n",
    "## 0.2 Theoretical Prerequisite: Probability and Statistics\n",
    "\n",
    "Risk measurement relies fundamentally on understanding the distribution of returns.\n",
    "\n",
    "### Key Concepts to Recall\n",
    "\n",
    "- **Mean (μ):** The average return; our estimate of expected return.  \n",
    "- **Variance (σ²) & Standard Deviation (σ):** The spread of returns around the mean, defining volatility (risk).  \n",
    "- **Skewness:** Measures the asymmetry of the distribution (negative skew is bad, indicating frequent small gains but rare large losses).  \n",
    "- **Kurtosis:** Measures the \"tailedness\" or \"peakedness\" of the distribution. High (excess) kurtosis indicates fat tails, meaning extreme events are more likely than predicted by the Normal Distribution.\n",
    "\n",
    "### Review Exercise: QQ-Plots and Normality\n",
    "\n",
    "The Quantile-Quantile (QQ) Plot is the primary visual tool to check if a dataset follows a normal distribution. \n",
    "If the data points hug the 45-degree line, the assumption holds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ccde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Use the log returns from the previous exercise\n",
    "np.random.seed(42)\n",
    "normal_sample = np.random.normal(0, log_returns_ex.std(), 100)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "stats.probplot(normal_sample, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ-Plot: Normal Sample (Expected)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb378d8",
   "metadata": {},
   "source": [
    "\n",
    "# Week 0: Advanced Risk Analysis Lecture  \n",
    "### Using Python, yfinance, and Statistical Foundations  \n",
    "\n",
    "This lecture introduces key concepts in **financial risk measurement** and **quantitative portfolio analysis**.  \n",
    "By the end of this session, students will understand:\n",
    "\n",
    "- The meaning of *financial risk* and its different forms (systematic vs idiosyncratic)\n",
    "- How to compute and interpret *volatility*, *covariance*, and *risk-return ratios*\n",
    "- How to use the **yfinance** API to pull real asset data for empirical tests\n",
    "- Applications of *Value at Risk (VaR)* and *Conditional VaR (CVaR)*\n",
    "- Practical coding skills for **portfolio diversification** and **tail risk management**\n",
    "\n",
    "> **Tip:** Run all cells in order to ensure that data dependencies and plots render correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3a68a",
   "metadata": {},
   "source": [
    "# Week 0 — Advanced Risk Analysis (Lecture-style)\n",
    "\n",
    "This notebook is a lecture-style, in-depth Week 0 module for an advanced course in Risk Analysis. It uses real market data via `yfinance` to demonstrate key concepts. Run the cells top-to-bottom in Jupyter.\n",
    "\n",
    "**Contents **:\n",
    "- Intro: why risk matters\n",
    "- Statistical foundations (LLN, CLT, tails)\n",
    "- Volatility and risk-return metrics\n",
    "- Portfolio risk: covariance, diversification, beta\n",
    "- Tail risk: VaR, CVaR, EVT intro\n",
    "- Practical labs using `yfinance` (data fetch, computations, plots)\n",
    "- Exercises & further reading\n",
    "\n",
    "**Note:** This notebook fetches data using `yfinance`. Ensure you have internet access when running the fetch cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6d1f7",
   "metadata": {},
   "source": [
    "## 1. The Nature of Financial Risk — Why this matters\n",
    "\n",
    "- Financial markets are driven by uncertainty. Risk measurement underpins portfolio construction, risk limits, and regulatory capital.\n",
    "\n",
    "**Lecture note:** Real-world risk is not just volatility — it includes liquidity, credit, and systemic components. Throughout this course we focus on **market & tail risk** and tools to measure them.\n",
    "\n",
    "**Case study (motivation):** 2008 Global Financial Crisis — common risk measures failed because of hidden correlations, leverage, and extreme tail events. We'll see why classical Gaussian models mislead risk managers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0602e",
   "metadata": {},
   "source": [
    "\n",
    "## Statistical Foundations: LLN & CLT\n",
    "\n",
    "The **Law of Large Numbers (LLN)** states that:\n",
    "\\[\n",
    "\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\xrightarrow{p} \\mu\n",
    "\\]\n",
    "as \\( n \\to \\infty \\). This implies that sample averages converge to the population mean.\n",
    "\n",
    "The **Central Limit Theorem (CLT)** states:\n",
    "\\[\n",
    "\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\n",
    "\\]\n",
    "meaning the standardized sample mean converges in distribution to a normal distribution.\n",
    "\n",
    "However, in financial data:\n",
    "- Returns exhibit **skewness** and **excess kurtosis**.\n",
    "- This violates the normality assumption underlying CLT-based risk models.\n",
    "- Heavy tails imply larger-than-expected extreme events (e.g., crashes).\n",
    "\n",
    "We observe this empirically through QQ-plots and tail estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddb5c2",
   "metadata": {},
   "source": [
    "## 2. Data Fetch (yfinance)\n",
    "\n",
    "This notebook will use daily prices for several tickers (e.g., SPY, AAPL, MSFT, GLD, TLT). Run the cell below to fetch data. If `yfinance` is not installed, install with `pip install yfinance`.\n",
    "\n",
    "**Important:** If you are behind a firewall, ensure your environment can access Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Why use log returns?\n",
    "# Log returns are additive over time (ln(P_t/P_{t-1})) and simplify compounding.\n",
    "# They also stabilize variance and are more appropriate for statistical modeling.\n",
    "# In financial econometrics, they are preferred for aggregation and for applying CLT-based tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data fetch using yfinance\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "tickers = ['SPY', 'AAPL', 'MSFT', 'GLD', 'TLT']\n",
    "start = '2015-01-01'\n",
    "end = None\n",
    "\n",
    "# Fetch adjusted close prices\n",
    "data = yf.download(tickers, start=start, end=end, progress=False)['Adj Close']\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f73ddb",
   "metadata": {},
   "source": [
    "## 3. Returns, Log-returns, and Summary Statistics\n",
    "\n",
    "Compute simple returns and log-returns. We show mean, volatility, skewness, and kurtosis. Lecture note: use log-returns for aggregation over time; simple returns are intuitive for percentage changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f06042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Compute daily simple returns and log returns\n",
    "returns = data.pct_change().dropna()\n",
    "log_returns = np.log(data).diff().dropna()\n",
    "\n",
    "# Summary statistics\n",
    "stats = returns.describe().T\n",
    "stats['skew'] = returns.apply(skew)\n",
    "stats['kurtosis'] = returns.apply(kurtosis)\n",
    "stats['annualized_vol'] = returns.std() * (252**0.5)\n",
    "stats[['mean','annualized_vol','skew','kurtosis']].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28799c",
   "metadata": {},
   "source": [
    "## 4. Visuals: Price Series, Return Distributions, QQ-plots\n",
    "\n",
    "Visual intuition is crucial. Below we plot price series, rolling volatility, histograms, and QQ-plots to inspect normality and tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hill estimator intuition:\n",
    "# The Hill estimator is used to estimate the tail index of heavy-tailed distributions.\n",
    "# A smaller tail index implies fatter tails — more extreme risk events.\n",
    "# This helps in understanding how much worse than \"normal\" the extreme outcomes can be.\n",
    "# It's particularly useful for stress testing and systemic risk measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "data.plot(title='Adjusted Close Prices', figsize=(12,6))\n",
    "plt.show()\n",
    "\n",
    "# Rolling volatility\n",
    "rolling_vol = returns['SPY'].rolling(window=21).std() * (252**0.5)\n",
    "rolling_vol.plot(title='21-day Rolling Annualized Volatility (SPY)')\n",
    "plt.show()\n",
    "\n",
    "# Histogram + QQ for SPY returns\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(returns['SPY'], bins=80, kde=True)\n",
    "plt.title('SPY Daily Returns Histogram')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sm.qqplot(returns['SPY'], line='s')\n",
    "plt.title('QQ-plot SPY vs Normal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef5781",
   "metadata": {},
   "source": [
    "## 5. Volatility, Sharpe Ratio, and Drawdowns\n",
    "\n",
    "Definitions and practical computation. We compute portfolio metrics for an equally-weighted portfolio of the tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio equal-weight example\n",
    "weights = np.repeat(1/len(tickers), len(tickers))\n",
    "portfolio_returns = returns.dot(weights)\n",
    "\n",
    "annualized_return = (1+portfolio_returns.mean())**252 - 1\n",
    "annualized_vol = portfolio_returns.std() * (252**0.5)\n",
    "sharpe = (annualized_return - 0.01) / annualized_vol  # assume 1% rf\n",
    "\n",
    "# Drawdown computation\n",
    "cum_returns = (1 + portfolio_returns).cumprod()\n",
    "running_max = cum_returns.cummax()\n",
    "drawdown = (cum_returns - running_max) / running_max\n",
    "\n",
    "print(f\"Annualized Return: {annualized_return:.3%}\")\n",
    "print(f\"Annualized Volatility: {annualized_vol:.3%}\")\n",
    "print(f\"Sharpe Ratio (rf=1%): {sharpe:.2f}\")\n",
    "print(f\"Max Drawdown: {drawdown.min():.2%}\")\n",
    "\n",
    "cum_returns.plot(title='Portfolio Cumulative Returns', figsize=(12,5))\n",
    "drawdown.plot(title='Portfolio Drawdown', figsize=(12,4), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80502c62",
   "metadata": {},
   "source": [
    "## 6. Portfolio Variance and Covariance Matrix\n",
    "\n",
    "Derive portfolio variance: \\(\\sigma_p^2 = w^\\top \\Sigma w\\). We display covariance and correlation heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f63635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VaR (Value-at-Risk) and ES (Expected Shortfall) Intuition:\n",
    "# VaR answers: \"What is the worst expected loss at a given confidence level (e.g., 95%)?\"\n",
    "# ES answers: \"If we exceed VaR, how large are the average losses beyond that point?\"\n",
    "# These are crucial for capital allocation, risk management, and regulatory reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f79051",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = returns.cov()\n",
    "corr = returns.corr()\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, cmap='vlag')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b0574",
   "metadata": {},
   "source": [
    "## 7. Beta Estimation & CAPM\n",
    "\n",
    "Estimate beta for each asset relative to SPY using OLS. Discuss interpretation: beta as sensitivity to market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "market = returns['SPY']\n",
    "betas = {}\n",
    "for col in returns.columns:\n",
    "    if col == 'SPY':\n",
    "        continue\n",
    "    y = returns[col]\n",
    "    X = sm.add_constant(market)\n",
    "    res = sm.OLS(y, X).fit()\n",
    "    betas[col] = res.params[1]\n",
    "betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116df8a",
   "metadata": {},
   "source": [
    "## 8. Value at Risk (VaR) and Conditional VaR (CVaR)\n",
    "\n",
    "Compute historical VaR, parametric (Gaussian) VaR, and CVaR for portfolio. Discuss pros/cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53959b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 0.01  # 99% VaR level as lower tail\n",
    "# Historical VaR\n",
    "VaR_hist = -np.quantile(portfolio_returns, alpha)\n",
    "# Parametric VaR (Gaussian)\n",
    "mu = portfolio_returns.mean()\n",
    "sigma = portfolio_returns.std()\n",
    "z = np.quantile(np.random.normal(size=1000000), alpha)\n",
    "VaR_param = -(mu + z * sigma)\n",
    "# CVaR (Expected Shortfall) historical\n",
    "losses = -portfolio_returns\n",
    "CVaR_hist = losses[losses >= VaR_hist].mean()\n",
    "\n",
    "print(f\"Historical VaR (99%): {VaR_hist:.4%}\")\n",
    "print(f\"Parametric VaR (99%): {VaR_param:.4%}\")\n",
    "print(f\"Historical CVaR (99%): {CVaR_hist:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b4a7d",
   "metadata": {},
   "source": [
    "## 9. Backtesting VaR (Kupiec Test)\n",
    "\n",
    "Implement a simple Kupiec proportion-of-failures test to see whether VaR breaches match expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test\n",
    "\n",
    "# Kupiec test (unconditional coverage)\n",
    "breaches = (portfolio_returns < -VaR_hist).sum()\n",
    "n_obs = len(portfolio_returns)\n",
    "p_hat = breaches / n_obs\n",
    "p0 = alpha\n",
    "\n",
    "print(f\"Breaches: {breaches} / {n_obs} (observed rate={p_hat:.4f})\")\n",
    "# two-sided binomial test (exact)\n",
    "pvalue = binom_test(breaches, n_obs, p0, alternative='two-sided')\n",
    "print(f\"Kupiec (binomial) p-value: {pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fae31b",
   "metadata": {},
   "source": [
    "## 10. Tail Analysis: QQ-plots and Intro to EVT\n",
    "\n",
    "Use QQ-plots and empirical tail index plots. Introduce Peaks Over Threshold and GPD for later weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac7ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "# QQ-plot for portfolio returns\n",
    "qqplot(portfolio_returns, line='s')\n",
    "plt.title('QQ-plot: Portfolio Returns vs Normal')\n",
    "plt.show()\n",
    "\n",
    "# Empirical tail index (Hill-ish) simple diagnostic for SPY\n",
    "sorted_losses = np.sort(-returns['SPY'])[::-1]  # descending losses\n",
    "k = 500\n",
    "hill_est = np.mean(np.log(sorted_losses[:k]) - np.log(sorted_losses[k]))\n",
    "print(f\"Rough Hill estimate (k={k}): {1/hill_est if hill_est>0 else np.nan:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd042da4",
   "metadata": {},
   "source": [
    "## 11. Scenario Analysis & Stress Testing\n",
    "\n",
    "Demonstrate stress scenarios: sudden volatility spike; correlation going to 0.9; market crash scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress scenario: market crash and correlation spike\n",
    "# 1) Market crash: -10% on SPY day (shock)\n",
    "shock = -0.10\n",
    "# approximate impact on portfolio: revalue by applying same shock proportionally to holdings via betas\n",
    "approx_port_loss = sum([betas.get(ticker, 1.0) * shock * w for ticker, w in zip(returns.columns, [1/len(returns.columns)]*len(returns.columns))])\n",
    "print(f\"Approx portfolio loss from -10% market shock (beta-weighted): {approx_port_loss:.2%}\")\n",
    "\n",
    "# 2) Correlation shock: recompute portfolio volatility with high correlation\n",
    "high_corr = corr.copy()\n",
    "high_corr_values = high_corr.values\n",
    "high_corr_values[high_corr_values<1] = 0.9  # set off-diagonals to 0.9 roughly\n",
    "# rebuild covariance assuming same vol but high corr (simplified)\n",
    "vols = returns.std().values\n",
    "cov_high = np.outer(vols, vols) * 0.9\n",
    "w = np.repeat(1/len(vols), len(vols))\n",
    "sigma_p_high = (w @ cov_high @ w)**0.5\n",
    "print(f\"Portfolio vol under high-correlation scenario: {sigma_p_high* (252**0.5):.2%} annualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2792e",
   "metadata": {},
   "source": [
    "## 12. Visualization Summary\n",
    "\n",
    "Key visual aids included: price series, rolling volatility, histograms, QQ-plots, correlation heatmap, drawdowns. These are essential for communicating risk to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed992d8",
   "metadata": {},
   "source": [
    "## 13. Exercises (Detailed)\n",
    "\n",
    "1. Compute parametric VaR and historical VaR for each individual asset and compare which assets are underestimated by parametric VaR.\n",
    "\n",
    "2. Implement bootstrapped confidence intervals for the historical VaR of the portfolio (95% CI).\n",
    "\n",
    "3. Fit a t-distribution to SPY returns and compare 99.9% quantile to Gaussian.\n",
    "\n",
    "4. Create a function that shocks the correlation matrix and find the worst-case VaR subject to the covariance matrix remaining positive semidefinite.\n",
    "\n",
    "5. (Advanced) Implement POT/GPD fit for SPY returns and compute 1-day 99.9% VaR using EVT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae1586",
   "metadata": {},
   "source": [
    "## 14. Further Reading & References\n",
    "\n",
    "- McNeil, Frey & Embrechts — *Quantitative Risk Management* (Chapters on VaR, EVT, Copulas)\n",
    "- Embrechts et al. — *Modelling Extremal Events*\n",
    "- Cont — *Empirical properties of asset returns*\n",
    "- Rockafellar & Uryasev — *Optimization of CVaR*\n",
    "\n",
    "---\n",
    "End of Week 0 lecture notebook. Move to Week 1 for Portfolio Risk & Correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0eb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated returns as placeholder\n",
    "data = np.random.normal(0, 0.02, 1000)\n",
    "\n",
    "@interact(alpha=(0.90, 0.99, 0.999))\n",
    "def show_var(alpha=0.95):\n",
    "    var_est = -np.quantile(data, 1 - alpha)\n",
    "    plt.hist(data, bins=40, color='lightblue', edgecolor='k')\n",
    "    plt.axvline(-var_est, color='red', linestyle='--', label=f'VaR@{alpha*100:.1f}%')\n",
    "    plt.legend()\n",
    "    plt.title(f'Interactive VaR Visualization (alpha={alpha:.3f})')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef641c7",
   "metadata": {},
   "source": [
    "\n",
    "# Summary and Discussion Points\n",
    "\n",
    "This lecture demonstrated how risk measures bridge theory and practice. Recall the following:\n",
    "\n",
    "1. **Risk and Return** are two sides of the same coin — expected return compensates for uncertainty.\n",
    "2. **Volatility** captures dispersion, but not direction or tail heaviness.\n",
    "3. **Covariance Structure** guides portfolio construction; diversification is not elimination of risk, but management of exposure.\n",
    "4. **Tail Metrics (VaR/CVaR)** are essential under non-Gaussian distributions to anticipate extreme losses.\n",
    "\n",
    "---\n",
    "### Real-World Extensions\n",
    "\n",
    "- Implement rolling-window VaR/CVaR to explore time-varying risk.\n",
    "- Compare historical VaR estimates to parametric ones.\n",
    "- Integrate macroeconomic indicators (e.g., VIX, interest rates) for a multi-factor risk model.\n",
    "\n",
    "---\n",
    "**Exercise:** Download price series for three additional ETFs and compute portfolio VaR using your own weighting scheme. Discuss how diversification affects tail risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede6a8a",
   "metadata": {},
   "source": [
    "\n",
    "## Enhanced VaR & CVaR: Student's t and Gaussian CVaR\n",
    "\n",
    "**Motivation:** The Gaussian VaR often underestimates extreme losses when returns exhibit heavy tails.\n",
    "We add a Student's *t* parametric VaR (fit to data) and an analytical Gaussian CVaR to compare with Historical estimates.\n",
    "\n",
    "**What students should learn:**  \n",
    "- How to fit a Student's *t* distribution to returns and compute the corresponding VaR.  \n",
    "- How Gaussian analytical CVaR differs from empirical CVaR and why it can understate tail risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d648c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- t-VaR and Gaussian CVaR calculation ---\n",
    "# This cell assumes `portfolio_returns` (daily simple returns) is already defined in the notebook.\n",
    "# If not, it will fetch data for an equally-weighted portfolio of example tickers.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import yfinance as yf\n",
    "\n",
    "try:\n",
    "    portfolio_returns  # check if exists\n",
    "except NameError:\n",
    "    # recreate a simple portfolio if variable not present\n",
    "    tickers = ['SPY','AAPL','MSFT','GLD','TLT']\n",
    "    data = yf.download(tickers, start='2015-01-01', progress=False)['Adj Close'].dropna()\n",
    "    returns = data.pct_change().dropna()\n",
    "    weights = np.repeat(1/len(tickers), len(tickers))\n",
    "    portfolio_returns = returns.dot(weights)\n",
    "\n",
    "alpha = 0.01  # 99% VaR (lower tail alpha)\n",
    "mu = portfolio_returns.mean()\n",
    "sigma = portfolio_returns.std(ddof=1)\n",
    "\n",
    "# Historical VaR/ES\n",
    "VaR_hist = -np.quantile(portfolio_returns, alpha)\n",
    "ES_hist = -portfolio_returns[portfolio_returns <= np.quantile(portfolio_returns, alpha)].mean()\n",
    "\n",
    "# Gaussian parametric VaR/ES (analytical CVaR)\n",
    "z = st.norm.ppf(alpha)\n",
    "VaR_gauss = -(mu + z * sigma)\n",
    "ES_gauss = -(mu + sigma * st.norm.pdf(z) / (1 - alpha))\n",
    "\n",
    "# Fit Student's t to portfolio returns (use scipy.stats.t.fit)\n",
    "# scipy returns (df, loc, scale)\n",
    "t_params = st.t.fit(portfolio_returns)\n",
    "df_t, loc_t, scale_t = t_params\n",
    "# t-VaR (use ppf)\n",
    "VaR_t = -st.t.ppf(alpha, df_t, loc=loc_t, scale=scale_t)\n",
    "# For t-ES (expected shortfall) we can approximate by numerical integration if desired\n",
    "# Here compute empirical ES under fitted t by sampling a large number of points\n",
    "rng = np.random.default_rng(12345)\n",
    "samples_t = st.t.rvs(df_t, loc=loc_t, scale=scale_t, size=200000, random_state=rng)\n",
    "ES_t_approx = -np.mean(samples_t[samples_t <= np.quantile(samples_t, alpha)])\n",
    "\n",
    "print(f\"Data points used: {len(portfolio_returns)}\")\n",
    "print(f\"Historical VaR (99%): {VaR_hist:.6f}, ES (empirical): {ES_hist:.6f}\")\n",
    "print(f\"Gaussian VaR (99%): {VaR_gauss:.6f}, Gaussian analytical ES: {ES_gauss:.6f}\")\n",
    "print(f\"Student-t fit params: df={df_t:.3f}, loc={loc_t:.6f}, scale={scale_t:.6f}\")\n",
    "print(f\"t-VaR (99%): {VaR_t:.6f}, t-ES approx (by sampling): {ES_t_approx:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253a192",
   "metadata": {},
   "source": [
    "\n",
    "## Systematic vs Idiosyncratic Risk Decomposition\n",
    "\n",
    "After estimating beta via OLS (CAPM regression), we can decompose an asset's total variance into:\n",
    "\\[\n",
    "\\sigma_{\\text{Total}}^2 = \\sigma_{\\text{Systematic}}^2 + \\sigma_{\\text{Idiosyncratic}}^2\n",
    "\\]\n",
    "where\n",
    "\\[\n",
    "\\sigma_{\\text{Systematic}}^2 = \\beta^2 \\sigma_{\\text{Market}}^2\n",
    "\\]\n",
    "\n",
    "We'll compute this decomposition for one asset (AAPL) and visualize the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Systematic vs Idiosyncratic variance decomposition ---\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure returns DataFrame exists; if not, fetch sample\n",
    "try:\n",
    "    returns  # should be a DataFrame of asset returns\n",
    "except NameError:\n",
    "    tickers = ['SPY','AAPL','MSFT','GLD','TLT']\n",
    "    data = yf.download(tickers, start='2015-01-01', progress=False)['Adj Close'].dropna()\n",
    "    returns = data.pct_change().dropna()\n",
    "\n",
    "market = returns['SPY']\n",
    "asset = returns['AAPL']\n",
    "\n",
    "# OLS regression: asset ~ market\n",
    "X = sm.add_constant(market)\n",
    "res = sm.OLS(asset, X).fit()\n",
    "beta = res.params[1]\n",
    "r2 = res.rsquared\n",
    "\n",
    "sigma_market = market.std(ddof=1)\n",
    "sigma_asset = asset.std(ddof=1)\n",
    "\n",
    "sigma_systematic_sq = beta**2 * sigma_market**2\n",
    "sigma_idio_sq = sigma_asset**2 - sigma_systematic_sq\n",
    "\n",
    "print(res.summary().as_text())\n",
    "print('\\nDecomposition:')\n",
    "print(f'beta = {beta:.4f}, R-squared = {r2:.4f}')\n",
    "print(f'total variance = {sigma_asset**2:.8f}')\n",
    "print(f'systematic variance = {sigma_systematic_sq:.8f}')\n",
    "print(f'idiosyncratic variance = {sigma_idio_sq:.8f}')\n",
    "\n",
    "# Visualize decomposition\n",
    "labels = ['Systematic', 'Idiosyncratic']\n",
    "vals = [sigma_systematic_sq, sigma_idio_sq]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(labels, vals, color=['#4c72b0','#dd8452'])\n",
    "plt.title('Variance Decomposition for AAPL (daily variance)')\n",
    "plt.ylabel('Variance (daily)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559be7e",
   "metadata": {},
   "source": [
    "\n",
    "##  Hill Estimator & Hill Plot\n",
    "\n",
    "The Hill estimator for the tail index depends on the choice of `k` (number of top order statistics).  \n",
    "A **Hill plot** shows the estimated tail index as a function of `k`. Look for ranges where the estimate stabilizes — those are plausible choices for `k`.\n",
    "\n",
    "We'll plot the Hill estimate for SPY losses and annotate the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ae963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Hill plot for SPY losses ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use negative returns (losses) for tail analysis\n",
    "try:\n",
    "    spy_losses = -returns['SPY'].dropna().values\n",
    "except NameError:\n",
    "    data = yf.download('SPY', start='2015-01-01', progress=False)['Adj Close'].dropna()\n",
    "    spy_returns = data.pct_change().dropna()\n",
    "    spy_losses = -spy_returns.values\n",
    "\n",
    "# Sort descending (largest losses first)\n",
    "sorted_losses = np.sort(spy_losses)[::-1]\n",
    "n = len(sorted_losses)\n",
    "ks = np.arange(10, min(2000, n//2), 10)\n",
    "\n",
    "hill_estimates = []\n",
    "for k in ks:\n",
    "    top_k = sorted_losses[:k]\n",
    "    threshold = sorted_losses[k]\n",
    "    # Hill estimator: 1/alpha_hat = (1/k) * sum_{i=1}^k log(X_i / X_{k+1})\n",
    "    logs = np.log(top_k / threshold)\n",
    "    hill_inv = np.mean(logs)\n",
    "    tail_index = 1 / hill_inv if hill_inv > 0 else np.nan\n",
    "    hill_estimates.append(tail_index)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ks, hill_estimates, marker='o', markersize=3)\n",
    "plt.xlabel('k (number of top order statistics)')\n",
    "plt.ylabel('Estimated tail index (alpha)')\n",
    "plt.title('Hill Plot (SPY losses) — look for stable regions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72729bc",
   "metadata": {},
   "source": [
    "\n",
    "##  Dynamic Risk-Free Rate via BIL ETF (proxy)\n",
    "\n",
    "Rather than hardcoding a risk-free rate, we fetch **BIL** (a 3-month T-bill ETF) and compute the realized annualized return over the sample window. We then use that rate to compute the Sharpe ratio more realistically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a734c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Fetch BIL and compute realized risk-free rate ---\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "start = returns.index[0].strftime('%Y-%m-%d') if 'returns' in globals() else '2015-01-01'\n",
    "end = returns.index[-1].strftime('%Y-%m-%d') if 'returns' in globals() else None\n",
    "\n",
    "bil = yf.download('BIL', start=start, end=end, progress=False)['Adj Close'].dropna()\n",
    "bil_returns = bil.pct_change().dropna()\n",
    "\n",
    "# annualized realized risk-free rate over the period\n",
    "rf_annual = (1 + bil_returns.mean())**252 - 1\n",
    "rf_daily = bil_returns.mean()\n",
    "\n",
    "print(f\"Sample period: {start} to {end}\")\n",
    "print(f\"Realized annualized risk-free rate (BIL proxy): {rf_annual:.4%}\")\n",
    "\n",
    "# Update Sharpe for portfolio using rf\n",
    "try:\n",
    "    ann_ret = (1 + portfolio_returns.mean())**252 - 1\n",
    "    ann_vol = portfolio_returns.std(ddof=1) * (252**0.5)\n",
    "    sharpe_real = (ann_ret - rf_annual) / ann_vol\n",
    "    print(f\"Portfolio annualized return: {ann_ret:.4%}, vol: {ann_vol:.4%}, Sharpe (BIL rf): {sharpe_real:.3f}\")\n",
    "except Exception as e:\n",
    "    print('Could not compute portfolio Sharpe with BIL rf:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a6e62",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##  Discussion Prompts & Teaching Notes\n",
    "\n",
    "1. **t-VaR vs Gaussian VaR:** For the sample, which VaR is largest? Why? Discuss parameter sensitivity and sample size issues.  \n",
    "2. **Hill plot selection:** Identify a stable region in the Hill plot. How would you choose `k` in practice? Consider bootstrap confidence bands.  \n",
    "3. **Systematic vs Idiosyncratic:** If idiosyncratic variance dominates for AAPL, what does that imply about diversification strategies?  \n",
    "4. **Risk-free proxy caveats:** Discuss when using an ETF proxy for the risk-free rate is reasonable and when it may be misleading (fees, tracking error, liquidity).\n",
    "\n",
    "---\n",
    "*Notebook updated on: 2025-10-15 22:51:12 UTC*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
